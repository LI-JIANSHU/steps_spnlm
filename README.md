Steps to put SPN language model into Kaldi, SWBD corpus. 

---
Prerequisite:
* The run.sh of SWBD has finished successfully.
* The SPN LM has been trained and the new ARPA file has been generated by [this script](https://github.com/LI-JIANSHU/deepasr/tree/master/SPNLM_Kaldi)

---
Usage:
* Download the script to the directory containing run.sh of SWBD: 
`git clone https://github.com/LI-JIANSHU/steps_spnlm.git`
* Make a soft link to the run_spnlm.sh file in the same directory of run.sh: 
`ln -s steps_spnlm/run_spnlm.sh run_spnlm.sh` 
* Run the script from the directory containing the run.sh

---
Tricks when running SWBD run.sh:

Kaldi's run.sh script is well paralleled. This could save the time to run the script, but also it will increase the work load tremendously. If the code is to be ran locally, the local machine may not handle such amount of work load parallelized by Kaldi. Even some sever with more CPU and memory resources would become irresponsive when running the code as it is. 

There are two levels of parallelism. For running the script locally, the first one is using the `run.pl` script to split one task into several parallel tasks. The `run.pl` script is a general purpose script to run any parallelizable task. One just need to specify the command, inputs, outputs and number of jobs (through `-nj` option) to it. The second one is using the command `&` to run a chunk of scripts in the background, when the outputs from that chunk doesn't affects the scripts after it. There are quite a lot of this background running when decoding for various language models, as their results are independent. 

If these two parallelism mechanisms are used simultaneously, the work load can be huge. If the machine can't handle this work load, the suggestion is that: Eliminate the second parallelism, and make the `-nj` option at most the number of CPUs. For SWBD, the size of data is huge compared to TIMIT. We do want to use parallel to make the best of the CPU resourses, but just have to make sure it won't crash the machine. 
